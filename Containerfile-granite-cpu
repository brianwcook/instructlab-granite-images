FROM registry.access.redhat.com/ubi9:9.4-1214.1726694543 AS env-build

WORKDIR /srv

# ARG MODEL_DOWNLOAD_URL

ARG MODEL_DOWNLOAD_URL
ARG MODEL_FILENAME
ARG HF_TOKEN
ARG LLAMA_CPP_SHA=1329c0a75e6a7defc5c380eaf80d8e0f66d7da78

# vulkan-headers vulkan-loader-devel vulkan-tools glslc glslang python3-pip mesa-libOpenCL-$MESA_VER.aarch64
RUN dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm && \
    crb enable && \
    dnf install -y epel-release && \
    dnf --enablerepo=ubi-9-appstream-rpms install -y git procps-ng vim \
      dnf-plugins-core python3-dnf-plugin-versionlock cmake gcc-c++ \
      python3-pip python3-argcomplete && \
    dnf copr enable -y slp/mesa-krunkit epel-9-$(uname -m) && \
    dnf install -y mesa-vulkan-drivers-23.3.3-102.el9 \
      vulkan-headers vulkan-loader-devel vulkan-tools spirv-tools glslc && \
    dnf clean all && \
    rm -rf /var/cache/*dnf*

# install build tools and clone and compile llama.cpp
RUN dnf -y update && dnf install -y git make automake gcc gcc-c++ llvm-toolset wget

RUN git clone --recursive https://github.com/ggerganov/llama.cpp && \
    cd llama.cpp && \
    git reset --hard ${LLAMA_CPP_SHA} && \
    cmake -B build -DCMAKE_INSTALL_PREFIX:PATH=/usr -DGGML_KOMPUTE=1 \
      -DGGML_CCACHE=0 && \
    cmake --build build --config Release -j $(nproc) && \
    cmake --install build && \
    cd / && \
    rm -rf llama.cpp

RUN wget --header="Authorization: Bearer ${HF_TOKEN}" ${MODEL_DOWNLOAD_URL} -P /models

# Runtime image
# FROM registry.access.redhat.com/ubi9-minimal:9.4-1227.1726694542 AS env-deploy
# # ARG MODEL_FILENAME
# RUN microdnf install shadow-utils
ENV LD_LIBRARY_PATH=/usr/local/lib

# COPY --from=0 /usr/lib64/libomp.so ${LD_LIBRARY_PATH}/libomp.so
# COPY --from=0 /usr/lib64/libllama.so ${LD_LIBRARY_PATH}/libllama.so
# COPY --from=0 /usr/lib64/libggml.so ${LD_LIBRARY_PATH}/libggml.so
# COPY --from=0 /usr/lib64/libgomp.so.1 ${LD_LIBRARY_PATH}/libgomp.so.1
# COPY --from=0 /usr/lib64/libfmt.so.10 ${LD_LIBRARY_PATH}/libfmt.so.10



# # copy llama.cpp binaries
# COPY --from=0 /usr/bin/llama-cli /usr/local/bin/llama-cli
# COPY --from=0 /usr/bin/llama-server /usr/local/bin/llama-server
# COPY --from=0 /models/granite-3.0-8b-instruct-Q4_K_M.gguf /models/granite-3.0-8b-instruct-Q4_K_M.gguf 

ENV MODEL_PATH=/models/${MODEL_FILENAME}
EXPOSE 8080

# copy and set entrypoint script
COPY entrypoint.sh /home/llama/entrypoint.sh

COPY granite-license /licenses/granite-license
COPY llama-cpp-license /licenses/llama-cpp-license

RUN ["chmod", "+x", "/home/llama/entrypoint.sh"]

RUN useradd --system --create-home llama

USER llama

WORKDIR /home/llama

ENTRYPOINT [ "/home/llama/entrypoint.sh" ]