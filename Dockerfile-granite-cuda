FROM nvidia/cuda:12.2.2-devel-ubi9 

WORKDIR /srv

# install build tools and clone and compile llama.cpp

RUN dnf -y update && dnf install -y git make automake gcc gcc-c++ llvm-toolset wget cmake libcurl-devel

ARG CUDA_DOCKER_ARCH=all

RUN export CMAKE_ARGS="-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}";

RUN git clone https://github.com/ggerganov/llama.cpp.git

RUN cd llama.cpp \
  && cmake -B build -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . && \
  cmake --build build --config Release -j$(nproc) && \
# FROM registry.access.redhat.com/ubi8/ubi:8.10-1088  AS env-deploy

# copy openmp and cuda libraries
# ENV LD_LIBRARY_PATH=/usr/local/lib


RUN cp /srv/llama.cpp/llama-cli /usr/local/bin/llama-cli
RUN cp /srv/llama.cpp/llama-server /usr/local/bin/llama-server

# create llama user and set home directory
RUN useradd --system --create-home llama

USER llama

WORKDIR /home/llama

EXPOSE 8080

# copy and set entrypoint script
COPY docker-entrypoint.sh /usr/local/bin/docker-entrypoint.sh

ENTRYPOINT [ "/usr/local/bin/docker-entrypoint.sh" ]